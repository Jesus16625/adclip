{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Disclaimer:** This colab is for experimental purposes with localization and the code here may be obsolete. Check our repository for the latest code: https://professional-services.googlesource.com/solutions/adclip\n",
        "\n",
        "**Last update:** September 2023\n",
        "\n",
        "# AdClip Locales\n",
        "\n",
        "This colab is the template for experimenting AdClip with Localization. Please make a copy of this.\n",
        "\n",
        "The template been tested with Thai and might need the customization to be applicable with another language.\n",
        "\n",
        "If there is any customized need for the language you're working on, please reach out to adclip-team@ to discuss.\n",
        "\n",
        "## How to use\n",
        "\n",
        "1. modify the Config part\n",
        "2. Run the cell from top to bottom\n",
        "3. Horizontal and Vertical videos will be created in colab local file `/content/`"
      ],
      "metadata": {
        "id": "t3bN4CnggFeI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Additional Resources\n",
        "\n",
        "Main Resources: [go/adclip](http://go/adclip)\n",
        "\n",
        "BRD: [go/adclip-brd](http://go/adclip-brd)\n",
        "\n",
        "PRD: [go/adclip-prd](http://go/adclip-prd)\n",
        "\n",
        "Experimental Deck: [go/adclip-localization](http://go/adclip-localization)"
      ],
      "metadata": {
        "id": "gZQOxcdIv3w9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1 Set up"
      ],
      "metadata": {
        "id": "u1pD7yVfgJsp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1.1 install dependencies\n",
        "\n",
        "After installation, please **always restart the runtime** before running other steps"
      ],
      "metadata": {
        "id": "aXeky79PlSc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install google-cloud-aiplatform\n",
        "!pip install google-cloud-speech\n",
        "!pip install firebase_functions~=0.1.0\n",
        "!pip install google-cloud-videointelligence\n",
        "!pip install moviepy\n",
        "!pip install pytube"
      ],
      "metadata": {
        "id": "BtlqUCvMlLDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1.2 Imports and Config"
      ],
      "metadata": {
        "id": "u0M5X3Cnlo3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize the imports\n",
        "from firebase_functions import https_fn\n",
        "from firebase_admin import initialize_app, firestore\n",
        "from google.cloud import speech, storage\n",
        "from vertexai.preview.language_models import TextGenerationModel\n",
        "from google.cloud import videointelligence\n",
        "\n",
        "import moviepy.editor as mpy\n",
        "import re\n",
        "import itertools\n",
        "import functools\n",
        "import copy\n",
        "import math\n",
        "import requests"
      ],
      "metadata": {
        "id": "-KB9ogID-QyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Config\n",
        "# GCP\n",
        "project_id = \"adclip\"               # replace with your own project\n",
        "location = \"us-central1\"            # always use us-central1 since asia-southeast1 (singapore) is not supported\n",
        "gcloud_bucket_name = \"adclip.appspot.com\"\n",
        "\n",
        "# youtube video\n",
        "youtube_video_url = \"https://youtu.be/KWEiXdb6gVY?si=aSNmCsW4M3feNAVZ\"\n",
        "video_name = \"acomu\"\n",
        "\n",
        "# language\n",
        "language_code = \"ja_JP\"                     # check the supported language and model here > https://cloud.google.com/speech-to-text/docs/speech-to-text-supported-languages\n",
        "video_transcript_model = \"Default\"          # if you use \"Default\" model, please take a closer look to text and timestamp since some word<>timestamp may not be 100% accurate"
      ],
      "metadata": {
        "id": "alXRLxDilkbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize GCP\n",
        "from google.colab import auth as google_auth\n",
        "\n",
        "google_auth.authenticate_user(project_id=project_id)\n",
        "!gcloud config set project {project_id}\n",
        "!gcloud config get-value project"
      ],
      "metadata": {
        "id": "S5UTRzRgoQWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Initialize Services\n",
        "storage_client = storage.Client()\n",
        "bucket = storage_client.get_bucket(gcloud_bucket_name)\n",
        "\n",
        "initialize_app()"
      ],
      "metadata": {
        "id": "WoN2ButMqUKe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c97d199-66c1-4107-8168-26ab5bd8e6f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<firebase_admin.App at 0x79333da22530>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2 Get the video from youtube (Optional)\n",
        "\n",
        "use the below code to get video from youtube. The video will be downloaded to `/content/{video_name}.mp4` colab local file\n",
        "\n",
        "Or you can upload your mp4 file into `/content/` directly and run \"upload to mp4 adclip bucket\" cell only."
      ],
      "metadata": {
        "id": "QMo_Azf_gMMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title download video\n",
        "from pytube import YouTube\n",
        "\n",
        "yt = YouTube(youtube_video_url)\n",
        "yt = yt.streams.filter(progressive=True, file_extension=\"mp4\").first()\n",
        "file_dir = yt.download(filename=f'{video_name}.mp4')\n",
        "print(f'file dir: {file_dir}')\n",
        "file_name = file_dir[9:len(file_dir)]\n",
        "print(f'file name: {file_name}')\n",
        "file_name_no_type = file_name[0:len(file_name)-4]\n",
        "print(f'file name no file type: {file_name_no_type}')"
      ],
      "metadata": {
        "id": "6EOooswEle8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title upload mp4 to adclip bucket\n",
        "\n",
        "# Upload file to the bucket.\n",
        "def upload_blob(source_file_name, destination_blob_name):\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print(f'File {source_file_name} uploaded to {destination_blob_name}')\n",
        "\n",
        "destination_blob_name = f'videos/{video_name}.mp4'\n",
        "source_file_name = file_dir\n",
        "\n",
        "upload_blob(source_file_name, destination_blob_name)"
      ],
      "metadata": {
        "id": "k7Im4LHypPxG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 Extract Audio"
      ],
      "metadata": {
        "id": "lb5mmHK1gTI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_audio(video_full_path, file_name, output_name = None):\n",
        "    file_name_without_extension = file_name.rsplit('.', 1)[0]\n",
        "    audio_output_file = (file_name_without_extension if output_name is None else output_name) + '.wav'\n",
        "    gcs_file_path = 'videos/audio/' + audio_output_file\n",
        "\n",
        "    if does_file_exist(gcs_file_path):\n",
        "        print('File {} exists'.format(gcs_file_path))\n",
        "        return f'gs://{gcloud_bucket_name}/{gcs_file_path}'\n",
        "\n",
        "    tmp_file_path = '/tmp/' + file_name\n",
        "\n",
        "    # use video file_path\n",
        "    blob = bucket.blob(video_full_path)\n",
        "    blob.download_to_filename(tmp_file_path)\n",
        "\n",
        "    clip = mpy.VideoFileClip(tmp_file_path)\n",
        "\n",
        "    audio_output_path = '/tmp/' + audio_output_file\n",
        "    clip.audio.write_audiofile(audio_output_path)\n",
        "\n",
        "    upload_blob(audio_output_path, gcs_file_path)\n",
        "    return f'gs://{gcloud_bucket_name}/{gcs_file_path}'\n",
        "\n",
        "def does_file_exist(filepath):\n",
        "  filename = filepath.split('/')[-1]\n",
        "  blob = bucket.get_blob(filepath)\n",
        "  return blob is not None"
      ],
      "metadata": {
        "id": "3TSw361Lm3-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extract_audio(video_full_path=f\"videos/{video_name}.mp4\", file_name=video_name)"
      ],
      "metadata": {
        "id": "lEfmvwogBk-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 Transcribe the video"
      ],
      "metadata": {
        "id": "F7eNnmuTg4QU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get overall \"pace\" or gap of every word in a clip then use twice the value of that to split the clips\n",
        "def does_file_exist(filepath):\n",
        "  filename = filepath.split('/')[-1]\n",
        "  blob = bucket.get_blob(filepath)\n",
        "  return blob is not None\n",
        "\n",
        "# Upload file to the bucket.\n",
        "def upload_blob(source_file_name, destination_blob_name):\n",
        "    blob = bucket.blob(destination_blob_name)\n",
        "    blob.upload_from_filename(source_file_name)\n",
        "\n",
        "    print('File {} uploaded to {}.'.format(\n",
        "        source_file_name,\n",
        "        destination_blob_name))\n",
        "\n",
        "def extract_audio(video_full_path, file_name, output_name = None):\n",
        "    file_name_without_extension = file_name.rsplit('.', 1)[0]\n",
        "    audio_output_file = (file_name_without_extension if output_name is None else output_name) + '.wav'\n",
        "    gcs_file_path = 'videos/audio/' + audio_output_file\n",
        "\n",
        "    if does_file_exist(gcs_file_path):\n",
        "        print('File {} exists'.format(gcs_file_path))\n",
        "        return f'gs://{gcloud_bucket_name}/{gcs_file_path}'\n",
        "\n",
        "    tmp_file_path = '/tmp/' + file_name\n",
        "\n",
        "    # use video file_path\n",
        "    blob = bucket.blob(video_full_path)\n",
        "    blob.download_to_filename(tmp_file_path)\n",
        "\n",
        "    clip = mpy.VideoFileClip(tmp_file_path)\n",
        "\n",
        "    audio_output_path = '/tmp/' + audio_output_file\n",
        "    clip.audio.write_audiofile(audio_output_path)\n",
        "\n",
        "    upload_blob(audio_output_path, gcs_file_path)\n",
        "    return f'gs://{gcloud_bucket_name}/{gcs_file_path}'\n",
        "\n",
        "# Get overall \"pace\" or gap of every word in a clip then use twice the value of that to split the clips\n",
        "def refine_transcript(transcript: list) -> list:\n",
        "    new_transcript = []\n",
        "\n",
        "    def generate_transcript_item(words: list) -> dict:\n",
        "        return {\n",
        "            \"text\": ' '.join(list(map(lambda word: word['text'], words))),\n",
        "            \"startTime\": words[0]['startTime'],\n",
        "            \"endTime\": words[-1]['endTime'],\n",
        "            \"duration\": words[-1]['endTime'] - words[0]['startTime'],\n",
        "            \"words\": words\n",
        "        }\n",
        "\n",
        "    for line in transcript:\n",
        "        gaps = list(map(lambda clip: clip['gap'], line['words']))\n",
        "        gaps.pop(0) #remove first gap\n",
        "\n",
        "        if len(gaps) == 0:\n",
        "          continue\n",
        "        average = sum(gaps) / len(gaps)\n",
        "        words = []\n",
        "        for index, word in enumerate(line['words']):\n",
        "            if index > 1 and word['gap'] > average * 2.5:\n",
        "                new_transcript.append(generate_transcript_item(words))\n",
        "                words = []\n",
        "\n",
        "            words.append(word)\n",
        "        if len(words) > 0:\n",
        "            new_transcript.append(generate_transcript_item(words))\n",
        "    return new_transcript\n",
        "\n",
        "# merge clips under 5seconds\n",
        "def merge_clips(transcript: list) -> list:\n",
        "    if len(transcript) == 0:\n",
        "        return []\n",
        "\n",
        "    def merge(transcript1, transcript2):\n",
        "        start_time = transcript1['startTime']\n",
        "        end_time = max(transcript1['endTime'], transcript2['endTime'])\n",
        "\n",
        "        return {\n",
        "            'text': f\"{transcript1['text']} {transcript2['text']}\",\n",
        "            'startTime': start_time,\n",
        "            'endTime': end_time,\n",
        "            'duration': end_time - start_time,\n",
        "            'words': transcript1['words'] + transcript2['words']\n",
        "        }\n",
        "\n",
        "    output = []\n",
        "    index = 0\n",
        "    clip = transcript[index]\n",
        "\n",
        "    def is_overlapping(transcript1, transcript2):\n",
        "        return False and (transcript2['words'][0]['startTime'] >=\n",
        "        transcript1['startTime'] and transcript2['words'][-1]['startTime'] <=\n",
        "        transcript1['endTime'])\n",
        "\n",
        "\n",
        "    for index in range(len(transcript)):\n",
        "        if index < len(transcript) - 1:\n",
        "            next = transcript[index + 1]\n",
        "            if (next['endTime'] - clip['startTime'] <= 5 or is_overlapping(\n",
        "            clip, next)):\n",
        "              clip = merge(clip, next)\n",
        "            else:\n",
        "              output.append(clip)\n",
        "              clip = transcript[index + 1]\n",
        "        else:\n",
        "            output.append(clip)\n",
        "\n",
        "    return output\n",
        "\n",
        "def refine_transcript2(file_name: str, video_gcs_uri: str, transcript: list) -> list:\n",
        "    new_transcript = []\n",
        "    print('refine_transcript2')\n",
        "    print(video_gcs_uri)\n",
        "    def generate_transcript_item(words: list, start_time: float, end_time: float) -> dict:\n",
        "        return {\n",
        "            \"text\": ' '.join(list(map(lambda word: word['text'], words))),\n",
        "            \"startTime\": start_time,\n",
        "            \"endTime\": end_time,\n",
        "            \"duration\": end_time - start_time,\n",
        "            \"words\": words\n",
        "        }\n",
        "\n",
        "    video_shots = get_video_shots(file_name)\n",
        "    if video_shots is None:\n",
        "        video_shots = process_video(video_gcs_uri)\n",
        "        upload_video_shots(file_name, video_shots)\n",
        "\n",
        "    video_shots_index = 0\n",
        "    list_of_words = list(map(lambda line: line['words'], transcript))\n",
        "    transcript_words = list(itertools.chain.from_iterable(list_of_words))\n",
        "    print('\\\\\\\\\\transcript_words////')\n",
        "    print(transcript_words)\n",
        "    words = []\n",
        "    for index, word in enumerate(transcript_words):\n",
        "        words.append(word)\n",
        "        while video_shots[video_shots_index]['end_time'] < words[0]['startTime']:\n",
        "            video_shots_index = video_shots_index + 1\n",
        "        video_shot = video_shots[video_shots_index]\n",
        "        if word['endTime'] > video_shot['end_time']:\n",
        "            start_time = min(words[0]['startTime'], video_shot['start_time'])\n",
        "            if index < len(transcript_words) - 1:\n",
        "                end_time = max(word['endTime'], min(video_shot['end_time'],\n",
        "                transcript_words[index+1]['startTime']))\n",
        "            else:\n",
        "                end_time = max(word['endTime'], video_shot['end_time'])\n",
        "            video_shots_index = video_shots_index + 1\n",
        "            end_time\n",
        "            new_transcript.append(generate_transcript_item(words, start_time, end_time))\n",
        "            words = []\n",
        "\n",
        "    if len(words) > 0:\n",
        "        start_time = min(words[0]['startTime'], video_shots[video_shots_index]['start_time'])\n",
        "        end_time = max(word['endTime'], video_shots[video_shots_index]['end_time'])\n",
        "        video_shots_index = video_shots_index + 1\n",
        "        new_transcript.append(generate_transcript_item(words, start_time, end_time))\n",
        "    return new_transcript\n",
        "\n",
        "def get_video_shots(file_name: str) -> bool:\n",
        "    db = firestore.client()\n",
        "    doc = db.collection('video_shots').document(file_name).get()\n",
        "\n",
        "    if not doc.exists:\n",
        "        return None\n",
        "\n",
        "    return doc.to_dict().get('data')\n",
        "\n",
        "def upload_video_shots(file_name: str, video_shots: list) -> None:\n",
        "    db = firestore.client()\n",
        "    doc_ref = db.collection('video_shots').document(file_name)\n",
        "    doc_ref.set({\"data\": video_shots})\n",
        "\n",
        "def get_transcript(file_name: str) -> bool:\n",
        "    try:\n",
        "      db = firestore.client()\n",
        "      doc = db.collection('transcripts').document(file_name).get()\n",
        "\n",
        "      if not doc.exists:\n",
        "        return None\n",
        "\n",
        "    except:\n",
        "      return None\n",
        "\n",
        "    return doc.to_dict().get('original')\n",
        "\n",
        "def upload_transcript(file_name: str, transcript: list) -> None:\n",
        "    db = firestore.client()\n",
        "    doc_ref = db.collection('transcripts').document(file_name)\n",
        "    doc_ref.set({\"original\": transcript})\n",
        "\n",
        "def transcribe_video(video_full_path, file_name, language_code, model) -> any:\n",
        "    if video_full_path is None:\n",
        "        return {\n",
        "            \"text\": \"Missing video uri, sample format: https://googleapis.com/Welcome to World Class.wav\"\n",
        "        }\n",
        "\n",
        "    transcript_in_firestore = get_transcript(file_name)\n",
        "    if transcript_in_firestore is not None:\n",
        "        return {\n",
        "            \"transcript\": merge_clips(refine_transcript2(file_name, f'gs://{gcloud_bucket_name}/{video_full_path}', transcript_in_firestore)),\n",
        "            \"original\": transcript_in_firestore,\n",
        "            \"v1\": refine_transcript(transcript_in_firestore),\n",
        "        }\n",
        "\n",
        "    # TODO: extract_audio from video (video_gcs_uri) then upload to GCS\n",
        "    audio_gcs_uri = extract_audio(video_full_path, file_name)\n",
        "\n",
        "\n",
        "    print(audio_gcs_uri)\n",
        "    audio = speech.RecognitionAudio(uri=audio_gcs_uri)\n",
        "\n",
        "    client = speech.SpeechClient()\n",
        "    config = speech.RecognitionConfig(\n",
        "        enable_word_time_offsets=True,\n",
        "        audio_channel_count=2, #2 is default for wav files\n",
        "        # Enable automatic punctuation\n",
        "        # enable_automatic_punctuation=True,\n",
        "        language_code=language_code,\n",
        "        # alternative_language_codes=alternate_languages,\n",
        "        model=model,\n",
        "        # Works only for model=\"video\" or \"phone call\"\n",
        "        use_enhanced=True\n",
        "    )\n",
        "\n",
        "    operation = client.long_running_recognize(config=config, audio=audio)\n",
        "\n",
        "    print(\"Waiting for operation to complete...\")\n",
        "    response = operation.result(timeout=900)\n",
        "\n",
        "    transcript_builder = []\n",
        "    last_end_time = 0\n",
        "    # Each result is for a consecutive portion of the audio. Iterate through\n",
        "    # them to get the transcripts for the entire audio file.\n",
        "    for result in response.results:\n",
        "        # The first alternative is the most likely one for this portion.\n",
        "        for alternative in result.alternatives:\n",
        "            print(f'len(words) = {len(alternative.words)}')\n",
        "            print(f'transcript: {alternative.transcript}')\n",
        "\n",
        "            if len(alternative.words) > 0:\n",
        "                transcript_item = {\n",
        "                    \"text\": alternative.transcript,\n",
        "                    \"startTime\": alternative.words[0].start_time.total_seconds(),\n",
        "                    \"endTime\": alternative.words[-1].end_time.total_seconds(),\n",
        "                    \"duration\": alternative.words[-1].end_time.total_seconds()\n",
        "                        - alternative.words[0].start_time.total_seconds()\n",
        "                }\n",
        "\n",
        "                transcript_item['words'] = []\n",
        "                for word in alternative.words:\n",
        "                    transcript_item['words'].append({\n",
        "                        \"text\": word.word,\n",
        "                        \"startTime\": word.start_time.total_seconds(),\n",
        "                        \"endTime\": word.end_time.total_seconds(),\n",
        "                        \"duration\": word.end_time.total_seconds()\n",
        "                            - word.start_time.total_seconds(),\n",
        "                        \"gap\": word.end_time.total_seconds() - last_end_time\n",
        "                    })\n",
        "                    last_end_time = word.end_time.total_seconds()\n",
        "                transcript_builder.append(transcript_item)\n",
        "\n",
        "    upload_transcript(file_name, transcript_builder)\n",
        "\n",
        "    return {\n",
        "        \"transcript\": merge_clips(refine_transcript2(file_name, f'gs://{gcloud_bucket_name}/{video_full_path}', transcript_builder)),\n",
        "        \"original\": transcript_builder,\n",
        "        \"v1\": refine_transcript(transcript_builder),\n",
        "        # \"transcript\": transcript_builder\n",
        "    }"
      ],
      "metadata": {
        "id": "-MR8qtRDuBw2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "video_full_path = f\"videos/{video_name}.mp4\"\n",
        "\n",
        "transcript = transcribe_video(video_full_path=video_full_path,\n",
        "                file_name=video_name,\n",
        "                language_code=language_code,\n",
        "                model=video_transcript_model)\n",
        "\n",
        "transcript"
      ],
      "metadata": {
        "id": "sepbaepxuEjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 Summarize video transcript"
      ],
      "metadata": {
        "id": "GnUwrHmHg8GR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_transcript(input_transcript, user_prompt, filename, max_duration=40, min_duration=10) -> any:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\summarize_transcript//////////')\n",
        "    max_duration = float(40)\n",
        "    min_duration = float(10)\n",
        "\n",
        "    full_text = '\\n'.join([x[\"text\"] for x in input_transcript])\n",
        "    print('----full_text-----')\n",
        "    print(full_text)\n",
        "\n",
        "    list_of_words = list(map(lambda line: line['words'], input_transcript))\n",
        "    transcript_words = list(itertools.chain.from_iterable(list_of_words))\n",
        "\n",
        "    video_shots = get_video_shots(filename)\n",
        "\n",
        "    # 1st attempt to shorten transcript\n",
        "    shortened_text = send_transcript_to_llm(text=make_prompt(full_text, user_prompt))\n",
        "    print('----shortened_text----')\n",
        "    print(shortened_text)\n",
        "\n",
        "    duration = calculate_duration(shortened_text, transcript_words, video_shots)\n",
        "    print('----duration----')\n",
        "    print(duration)\n",
        "\n",
        "    count = 0\n",
        "    # Validate duration and loop if condition is not met:\n",
        "    print(f'max_duration: {max_duration} and min_duration: {min_duration}')\n",
        "    while count < 3 and (duration > max_duration or duration < min_duration):\n",
        "        if duration < min_duration:\n",
        "            print('----shorter than min----')\n",
        "            shortened_text = send_transcript_to_llm(text=make_prompt(full_text, user_prompt))\n",
        "        else:\n",
        "            print('----longer than min----')\n",
        "            shortened_text = send_transcript_to_llm(text=make_prompt(shortened_text, user_prompt))\n",
        "        duration = calculate_duration(shortened_text, transcript_words, video_shots)\n",
        "        count += 1\n",
        "        print('----LOOP shortened_text----')\n",
        "        print(shortened_text)\n",
        "        print('----duration----')\n",
        "        print(duration)\n",
        "\n",
        "    upload_summary(full_text, shortened_text)\n",
        "    segments = get_clips_from_transcript(transcript_words, shortened_text)\n",
        "    print('----segments----')\n",
        "    print(segments)\n",
        "\n",
        "    segments = match_with_video_shots(video_shots, segments, transcript_words)\n",
        "    print('----segments + video shots----')\n",
        "    print(segments)\n",
        "\n",
        "    return  {\n",
        "        # \"summarized_transcript\": summarized_transcript\n",
        "        \"summarized_transcript\": segments\n",
        "    }\n",
        "\n",
        "def send_transcript_to_llm(text, model=\"text-bison@001\",\n",
        "                    temperature=0,max_output_tokens=300,top_k=40,top_p=0.8):\n",
        "    print('\\\\\\\\\\\\\\\\\\\\send_transcript_to_llm//////////')\n",
        "    model = TextGenerationModel.from_pretrained(model)\n",
        "    response = model.predict(text,\n",
        "        temperature=temperature,\n",
        "        max_output_tokens=max_output_tokens,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "    )\n",
        "    if response.text.lstrip().startswith('Transcript:\\n'):\n",
        "        return response.text.lstrip().replace('Transcript:\\n', '', 1)\n",
        "    print('----LLM Result----')\n",
        "    print(response.text)\n",
        "    return response.text\n",
        "\n",
        "def make_prompt(transcript, user_prompt = ''):\n",
        "    return f\"\"\"You are a senior copy writer for an advertising agency who excels at summarizing transcript for video ads.\n",
        "        Shorten the transcript by keeping important lines and removing other lines.\n",
        "        Keep the first and last lines of the transcript.\n",
        "        Keep the format of the output the same with the input.\n",
        "        Don't make it too short.\n",
        "        {user_prompt if type(user_prompt) == 'str' and len(user_prompt) > 0 else ''}\n",
        "\n",
        "        Transcript:\n",
        "        {transcript}\"\"\"\n",
        "\n",
        "def calculate_duration(shortened_text: str, transcript: list, video_shots: list) -> float:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\calculate_duration//////////')\n",
        "    total_duration = 0\n",
        "    clips = get_clips_from_transcript(transcript, shortened_text)\n",
        "    clips = match_with_video_shots(video_shots, clips, transcript)\n",
        "    for clip in clips:\n",
        "        total_duration += clip.get('duration')\n",
        "    print(f\"----total duration: {total_duration}----\")\n",
        "    return total_duration\n",
        "\n",
        "def match_with_video_shots(video_shots: str, transcript: list, words: list) -> list:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\match_with_video_shots//////////')\n",
        "    shot_index = 0\n",
        "    word_index = 0\n",
        "    for index, line in enumerate(transcript):\n",
        "        print(video_shots[shot_index]['end_time'])\n",
        "        print(line)\n",
        "        print(line['startTime'])\n",
        "        while video_shots[shot_index]['end_time'] < line['startTime']:\n",
        "            shot_index += 1\n",
        "        video_shot = video_shots[shot_index]\n",
        "\n",
        "        start_time = min(line['startTime'], video_shot['start_time'])\n",
        "        transcript[index]['startTime'] = start_time\n",
        "        print(f'start_time: {start_time}')\n",
        "\n",
        "        while video_shots[shot_index]['end_time'] < line['endTime']:\n",
        "            shot_index += 1\n",
        "        video_shot = video_shots[shot_index]\n",
        "\n",
        "        while (word_index < len(words) - 1 and words[word_index]['startTime']\n",
        "              < line['endTime']):\n",
        "            word_index += 1\n",
        "\n",
        "        end_time = max(line['endTime'], video_shot['end_time'])\n",
        "\n",
        "        if words[word_index]['endTime'] != line['endTime']:\n",
        "            end_time = max(line['endTime'], min(video_shot['end_time'],\n",
        "                words[word_index]['startTime']))\n",
        "        print(f'end_time: {end_time}')\n",
        "        transcript[index]['endTime'] = end_time\n",
        "        transcript[index]['duration'] = end_time - start_time\n",
        "\n",
        "    return transcript\n",
        "\n",
        "def get_video_shots(file_name: str) -> bool:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\get_video_shots//////////')\n",
        "    db = firestore.client()\n",
        "    doc = db.collection('video_shots').document(file_name).get()\n",
        "    print(doc)\n",
        "\n",
        "    if not doc.exists:\n",
        "      print('no video shot existed')\n",
        "      return None\n",
        "\n",
        "    print(doc.to_dict().get('data'))\n",
        "    return doc.to_dict().get('data')\n",
        "\n",
        "def upload_summary(full_text: str, summary: str) -> None:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\upload_summary//////////')\n",
        "    db = firestore.client()\n",
        "    doc_ref = db.collection('summary').document()\n",
        "    doc_ref.set({\"full_text\": full_text, \"summary\": summary, \"summary_repr\": repr(summary)})\n",
        "    print(doc_ref)\n",
        "\n",
        "def extract_words_from_str(summary: str) -> list:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\extract_words_from_str//////////')\n",
        "    print(f'summary: {summary}')\n",
        "    index = 0\n",
        "    transcript_ptr = 0\n",
        "    output = []\n",
        "    if summary.lstrip().lower().startswith('transcript:'):\n",
        "        summary = summary.lower().replace('transcript:', '', 1)\n",
        "\n",
        "    summary = re.sub('[,.?!]', '', summary).lower()\n",
        "    summary = summary.replace('\\n', ' ')\n",
        "\n",
        "    words = summary.split(' ')\n",
        "    words = list(filter(lambda word: len(word) > 0, words))\n",
        "    print(f'words: {words}')\n",
        "    return words\n",
        "\n",
        "def get_clips_from_transcript(transcript: list, summary: str) -> list:\n",
        "    print('\\\\\\\\\\\\\\\\\\\\get_clips_from_transcript//////////')\n",
        "    index = 0\n",
        "    transcript_ptr = 0\n",
        "    output = []\n",
        "    if summary.lstrip().lower().startswith('transcript:'):\n",
        "        summary = summary.lower().replace('transcript:', '', 1)\n",
        "\n",
        "    summary = re.sub('[,.?!]', '', summary).lower()\n",
        "    summary = summary.replace('\\n', ' ')\n",
        "\n",
        "    words = summary.split(' ')\n",
        "    words = list(filter(lambda word: len(word) > 0, words))\n",
        "\n",
        "    print(words)\n",
        "    word_ptr = 0\n",
        "    while word_ptr < len(words) and transcript_ptr < len(transcript):\n",
        "        transcript_builder = []\n",
        "\n",
        "        while (transcript_ptr < len(transcript) and\n",
        "        word_ptr < len(words) and\n",
        "        transcript[transcript_ptr].get('text').lower() != words[word_ptr]):\n",
        "            transcript_ptr = transcript_ptr + 1\n",
        "\n",
        "        while ((transcript_ptr < len(transcript) and\n",
        "        word_ptr < len(words) and\n",
        "        transcript[transcript_ptr].get('text').lower() == words[word_ptr])\n",
        "        or (transcript_ptr < len(transcript) - 1 and\n",
        "        word_ptr < len(words) - 1 and\n",
        "        transcript[transcript_ptr+1].get('text').lower() == words[word_ptr+1])\n",
        "        or (transcript_ptr < len(transcript) - 2 and\n",
        "        word_ptr < len(words) - 1 and\n",
        "        transcript[transcript_ptr+2].get('text').lower() == words[word_ptr+1])):\n",
        "            transcript_builder.append(transcript[transcript_ptr])\n",
        "            if transcript[transcript_ptr].get('text').lower() != words[word_ptr]:\n",
        "                transcript_builder.append(transcript[transcript_ptr+1])\n",
        "                if transcript[transcript_ptr+1].get('text').lower() != words[word_ptr+1]:\n",
        "                    transcript_builder.append(transcript[transcript_ptr+2])\n",
        "                    transcript_ptr += 1\n",
        "\n",
        "                transcript_ptr += 1\n",
        "                word_ptr += 1\n",
        "\n",
        "            transcript_ptr += 1\n",
        "            word_ptr += 1\n",
        "\n",
        "        if len(transcript_builder) == 0:\n",
        "            continue\n",
        "        if len(transcript_builder) == 1:\n",
        "            word_ptr = word_ptr - 1\n",
        "            continue\n",
        "\n",
        "        new_text = list(map(lambda item: item.get('text'), transcript_builder))\n",
        "        output.append({\n",
        "            'text': ' '.join(new_text),\n",
        "            'startTime': transcript_builder[0].get(\"startTime\"),\n",
        "            'endTime': transcript_builder[-1].get(\"endTime\"),\n",
        "            'duration': (transcript_builder[-1].get(\"endTime\") -\n",
        "                         transcript_builder[0].get(\"startTime\")),\n",
        "            'words': transcript_builder\n",
        "        })\n",
        "\n",
        "    return output\n",
        "\n",
        "def upload_video_shots(file_name: str, video_shots: list) -> None:\n",
        "    db = firestore.client()\n",
        "    doc_ref = db.collection('video_shots').document(file_name)\n",
        "    doc_ref.set({\"data\": video_shots})\n",
        "\n",
        "def process_video(video_gcs_uri: str, output_uri: str = None) -> list:\n",
        "    video_client = videointelligence.VideoIntelligenceServiceClient()\n",
        "\n",
        "    features = [\n",
        "        videointelligence.Feature.SHOT_CHANGE_DETECTION,\n",
        "        #videointelligence.Feature.SPEECH_TRANSCRIPTION,\n",
        "    ]\n",
        "\n",
        "    transcript_config = videointelligence.SpeechTranscriptionConfig(\n",
        "        language_code=\"en-US\"\n",
        "    )\n",
        "    video_context = videointelligence.VideoContext(\n",
        "        speech_transcription_config=transcript_config)\n",
        "\n",
        "    operation = video_client.annotate_video(\n",
        "        request={\"features\": features,\n",
        "                \"input_uri\": video_gcs_uri,}\n",
        "                # \"output_uri\": output_uri,\n",
        "    )\n",
        "\n",
        "    print(\"\\nProcessing video.\", operation)\n",
        "\n",
        "    result = operation.result(timeout=300)\n",
        "\n",
        "    print(\"\\n finished processing.\")\n",
        "\n",
        "    video_shots = []\n",
        "    # first result is retrieved because a single video was processed\n",
        "    for i, shot in enumerate(result.annotation_results[0].shot_annotations):\n",
        "        start_time = (\n",
        "            shot.start_time_offset.seconds + shot.start_time_offset.microseconds / 1e6\n",
        "        )\n",
        "        end_time = (\n",
        "            shot.end_time_offset.seconds + shot.end_time_offset.microseconds / 1e6\n",
        "        )\n",
        "        video_shots.append({\n",
        "            'start_time': math.floor(start_time * 10) / 10.0,\n",
        "            'end_time': round(end_time, 1),\n",
        "        })\n",
        "        print(\"\\tShot {}: {} to {}\".format(i, start_time, end_time))\n",
        "\n",
        "    return video_shots"
      ],
      "metadata": {
        "id": "-ztkF9aTuegO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Test with main function >> summarize_transcript\n",
        "user_prompt = ''     # modify this user prompt or leave it blank. (same as prompt input from AdClip UI)\n",
        "summarized_transcript = summarize_transcript(transcript['transcript'], user_prompt, video_name)\n",
        "\n",
        "segments = summarized_transcript['summarized_transcript']\n",
        "\n",
        "print(segments)"
      ],
      "metadata": {
        "id": "iku853Zb1IQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you got an empty transcript from cell above, try this >> Test step by step (one iteration of summarize_transcript)\n",
        "\n",
        "if the duration is 0, the rest of the code will be failed because it cannot mapped the shorten text back to the original transcript.\n",
        "\n",
        "<br/>\n",
        "\n",
        "in Thai language, we use `match_summary_with_original_transcript` instead of `get_clips_from_transcript` to match the original transcript by comparing word by word from original transcript with the LLM Result.\n",
        "\n",
        "<br/>\n",
        "<br/>\n",
        "\n",
        "visualize explanation in [go/adclip-localization](http://go/adclip-localization) slide 24\n",
        "\n",
        "For example,\n",
        "\n",
        "The first sentence of original transcript: \"เนส กาแฟ เรด คัพ คั่ว อย่าง เชี่ยวชาญ\" >> breakdown by space to \"เนส\", \"กาแฟ\", \"เรด\", \"คัพ\", \"คั่ว\", \"อย่าง\", \"เชี่ยวชาญ\"\n",
        "\n",
        "LLM Result: \"Nescafe Red Cup คั่วอย่างเชี่ยวชาญ\" >> breakdown by space to \"Nescafe\", \"Red\", \"Cup\", \"คั่วอย่างเชี่ยวชาญ\"\n",
        "\n",
        "<br/>\n",
        "\n",
        "`match_summary_with_original_transcript` will compare\n",
        "\n",
        "\"Nescafe\" with \"เนส\", \"กาแฟ\", \"เรด\", ... => cannot find the match, go to next word\n",
        "\n",
        "\"Red\" with \"เนส\", \"กาแฟ\", \"เรด\", ... => cannot find the match, go to next word\n",
        "\n",
        "\"Cup\" with with \"เนส\", \"กาแฟ\", \"เรด\", ... => cannot find the match, go to next word\n",
        "\n",
        "\"คั่วอย่างเชี่ยวชาญ\" with \"เนส\", \"กาแฟ\", \"เรด\", ..., \"คั่ว\" => found \"คั่ว\" in the original transcript => consider matched! add the sentence from original transcript to the summarized transcript"
      ],
      "metadata": {
        "id": "1xDKaG96FzuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# new function to map LLM summary --> original transcript (not merge to Google Cloud Function yet)\n",
        "def match_summary_with_original_transcript(transcript_words, original_transcript):\n",
        "  list_of_words = list(map(lambda line: line['words'], original_transcript))\n",
        "  transcript_words = list(itertools.chain.from_iterable(list_of_words))\n",
        "  extracted_words = extract_words_from_str(shorten_transcript)\n",
        "\n",
        "  words_ptr = 0\n",
        "  original_ptr = 0\n",
        "  matched_result = []\n",
        "  is_matched = False\n",
        "  latest_added_scene = -1\n",
        "  while words_ptr < len(extracted_words):\n",
        "    if original_ptr >= len(original_transcript) and latest_added_scene < 0:\n",
        "      original_ptr = 0\n",
        "    print(f\"before original_ptr {original_ptr} latest_added_scene {latest_added_scene}\")\n",
        "    if original_ptr >= len(original_transcript) or original_ptr > latest_added_scene and latest_added_scene > 0:\n",
        "      original_ptr = latest_added_scene\n",
        "    print(f\"after original_ptr {original_ptr} latest_added_scene {latest_added_scene}\")\n",
        "    print(f'checking [{extracted_words[words_ptr]}] with [{original_transcript[original_ptr][\"text\"]}]')\n",
        "    while original_ptr < len(original_transcript):\n",
        "      is_matched = find_the_match(extracted_words[words_ptr], original_transcript[original_ptr]['words'])\n",
        "      if is_matched:\n",
        "        if already_in_final_result(matched_result, original_transcript[original_ptr]):\n",
        "          break\n",
        "        else:\n",
        "          to_add_transcript = original_transcript[original_ptr]\n",
        "          to_add_transcript['index'] = original_ptr\n",
        "          matched_result.append(to_add_transcript)\n",
        "          print(f'scene added >> latest_added_scene {latest_added_scene}')\n",
        "          latest_added_scene = original_ptr\n",
        "          is_matched = False\n",
        "        break\n",
        "      original_ptr += 1\n",
        "    words_ptr += 1\n",
        "  return matched_result\n",
        "\n",
        "def find_the_match(words, original_transcript_word_list):\n",
        "  for original_words in original_transcript_word_list:\n",
        "    if original_words['text'] in words:\n",
        "      print('matched')\n",
        "      return True\n",
        "  return False\n",
        "\n",
        "def already_in_final_result(final_result_list, transcript_to_add):\n",
        "  if len(final_result_list) == 0:\n",
        "    return False\n",
        "  for result in final_result_list:\n",
        "    if transcript_to_add['text'] == result['text']:\n",
        "      return True\n",
        "  return False"
      ],
      "metadata": {
        "id": "GFLOtj0NIV3-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('---------start------------')\n",
        "user_prompt = ''          # modify this user prompt or leave it blank. (same as prompt input from AdClip UI)\n",
        "\n",
        "full_text = '\\n'.join([x[\"text\"] for x in transcript[\"transcript\"]])\n",
        "shorten_transcript = send_transcript_to_llm(text=make_prompt(full_text,user_prompt))\n",
        "\n",
        "list_of_words = list(map(lambda line: line['words'], transcript[\"transcript\"]))\n",
        "transcript_words = list(itertools.chain.from_iterable(list_of_words))\n",
        "extracted_words = extract_words_from_str(shorten_transcript)\n",
        "video_shots = get_video_shots(f\"{video_name}.mp4\")\n",
        "if video_shots is None:\n",
        "  video_shots = process_video(f'gs://{gcloud_bucket_name}/videos/{video_name}.mp4')\n",
        "  upload_video_shots(file_name, video_shots)\n",
        "\n",
        "print('----------Calculate Duration----------')\n",
        "duration = calculate_duration(shorten_transcript, transcript_words, video_shots)\n",
        "print(f'duration: {duration}')\n",
        "\n",
        "print('----------LLM result----------')\n",
        "print(' '.join(extracted_words))\n",
        "\n",
        "print('----------Map with original transcript----------')\n",
        "segments = get_clips_from_transcript(transcript_words, shorten_transcript)\n",
        "\n",
        "# can try to use this instead if duration = 0 or cannot map the transcrip by get_clips_from_transcript\n",
        "# segments = match_summary_with_original_transcript(transcript_words, transcript[\"transcript\"])\n",
        "\n",
        "print('---------- mapped result ----------')\n",
        "for s in segments:\n",
        "  print(s['text'])\n",
        "\n",
        "print('----------Map with Video Shots----------')\n",
        "segments = match_with_video_shots(video_shots, segments, transcript[\"transcript\"])\n",
        "print(f'segments: {segments}')\n",
        "\n",
        "print('----------Final Result----------')\n",
        "for s in segments:\n",
        "  print(f\"{s['startTime']} {s['endTime']}\")\n",
        "  print(f\"{s['text']}\")"
      ],
      "metadata": {
        "id": "SiaFxxp61bvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 cut the video"
      ],
      "metadata": {
        "id": "lDNrchSXhArN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.video.fx.all import crop\n",
        "\n",
        "def clip_video(video_path, segments):\n",
        "    # loading original video\n",
        "    original_clip = mpy.VideoFileClip(video_path)\n",
        "\n",
        "    new_clip = {}\n",
        "\n",
        "    for segment in segments:\n",
        "        #make sure end_time does not exceed the video duration\n",
        "        end_time = min(segment['endTime'], original_clip.duration)\n",
        "        if new_clip:\n",
        "            new_clip = mpy.concatenate_videoclips([new_clip, original_clip.subclip(segment['startTime'], end_time)])\n",
        "        else:\n",
        "            new_clip = original_clip.subclip(segment['startTime'], end_time)\n",
        "\n",
        "    (w, h) = new_clip.size\n",
        "\n",
        "    crop_width = h * 9/16\n",
        "    crop_width = crop_width//2*2\n",
        "\n",
        "    x1, x2 = (w - crop_width)//2, (w+crop_width)//2\n",
        "    y1, y2 = 0, h\n",
        "    cropped_clip = crop(new_clip, x1=x1, y1=y1, x2=x2, y2=y2)\n",
        "\n",
        "    cropped_clip.write_videofile(f\"{video_name}-vertical.mp4\",audio_codec=\"aac\")\n",
        "    new_clip.write_videofile(f\"{video_name}-horizontal.mp4\",audio_codec=\"aac\") #put in audio_codec so the clip has sound"
      ],
      "metadata": {
        "id": "8iCzOqZF97uN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Cut the video\n",
        "\n",
        "clip_video(video_path=f\"/content/{video_name}.mp4\", segments=segments)"
      ],
      "metadata": {
        "id": "6W-8FQXn99hJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}